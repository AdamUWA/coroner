{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration for the QandA Module\n",
    "\n",
    "This notebook provides a straightforward guide on how to use the `QandA` object from the `qanda.py` module. The `QandA` object encapsulates a complete Retrieval-Augmented Generation (RAG) pipeline, allowing you to ask questions of a document and receive answers grounded in its content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "First, we import the necessary libraries and define the configuration parameters for our RAG pipeline. These parameters tell the `QandA` object which models to use, which document to load, and how to structure its prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from qanda import QandA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "FILE_PATH = Path(\"jsondata/Rodier-Finding.jsonl\")\n",
    "GEN_MODEL = \"gemma3\"\n",
    "EMBED_MODEL = \"mxbai-embed-large\"\n",
    "VDB = InMemoryVectorStore\n",
    "TOP_K = 3\n",
    "PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Context information is below.\\n\n",
    "    ---------------------\\n\n",
    "    {context}\\n\n",
    "    ---------------------\\n\n",
    "    Given the context information and not prior knowledge, answer the query.\\n\n",
    "    Query: {input}\\n\n",
    "    Answer:\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create an instance of the `QandA` object. This will load the models and process the specified document, preparing the pipeline for questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing, please wait...\n",
      "Loading jsondata\\Rodier-Finding.jsonl\n",
      "Question Answer chain ready.\n"
     ]
    }
   ],
   "source": [
    "qanda = QandA(gen_model=GEN_MODEL,\n",
    "              embed_model=EMBED_MODEL, \n",
    "              vdb=VDB,\n",
    "              file_path=FILE_PATH,\n",
    "              top_k=TOP_K,\n",
    "              prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Usage: Asking a Question\n",
    "\n",
    "The simplest way to use the object is to call the `ask()` method with your question. It will return the answer as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the coroner?\n",
      "Answer: Sarah Helen Linton, Deputy State Coroner.\n",
      "\n",
      "Question: Who is the deceased?\n",
      "Answer: Frank Edward Rodier is the deceased.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Who is the coroner?\",\n",
    "    \"Who is the deceased?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qanda.ask(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Usage: Verifying with Sources\n",
    "\n",
    "A key feature of RAG is transparency. To see the exact text chunks that the LLM used as context to generate an answer, set the `verbose` parameter to `True`. This is crucial for verifying the accuracy and trustworthiness of the response.\n",
    "\n",
    "The method will return a tuple: `(answer, sources)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the cause of death?\n",
      "Answer: The cause of death remains unascertained. The coroner determined it was likely an accident, but could not definitively say whether injuries from the rocks contributed to his death.\n",
      "\n",
      "--- Sources Used ---\n",
      "Source 1:\n",
      "  - Page: 6\n",
      "  - Document: data/Rodier-Finding.pdf\n",
      "  - Text: IS DEATH ESTABLISHED? 17. As is clear from the above; I am satisfied beyond reasonable doubt that Frank Rodier is deceased and that he died on 25 1975 in the sea after he was washed off the rocks whil...\n",
      "Source 2:\n",
      "  - Page: 3\n",
      "  - Document: data/Rodier-Finding.pdf\n",
      "  - Text: INTRODUCTION - 2 In my capacity as the Acting State Coroner, I determined on the basis of information provided by the WA Police in August 2023 that   there was   reasonable cause to suspect that Frank...\n",
      "Source 3:\n",
      "  - Page: 4\n",
      "  - Document: data/Rodier-Finding.pdf\n",
      "  - Text: [2024] WACOR 35 impediment; associated with events at the time of his birth. He had no known medical conditions that would have shortened his life span: 3...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the cause of death?\"\n",
    "answer, sources = qanda.ask(question, verbose=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "print(\"--- Sources Used ---\")\n",
    "\n",
    "for src in sources:\n",
    "    text_preview = src['text'].replace('\\n', ' ')[:200] + '...'\n",
    "    print(f\"Source {src['source']}:\")\n",
    "    print(f\"  - Page: {src['page']}\")\n",
    "    print(f\"  - Document: {src['document']}\")\n",
    "    print(f\"  - Text: {text_preview}\")\n",
    "\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "This notebook has demonstrated the core functionalities of the `qanda.py` module. By initializing the `QandA` object with the desired configuration, you can easily perform verifiable, source-grounded question answering on local documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
