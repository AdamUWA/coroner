{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Reproducible Evaluation Results\n",
    "\n",
    "**Audience:** Dr. Matthew Albrecht (WACRSR), Project Markers\n",
    "\n",
    "This notebook serves as a companion to the final project report, providing the live code and reproducible results that support our findings. It demonstrates the core functionality of the `QandA` system and validates the quantitative evaluation discussed in **Section 4** of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Initialization\n",
    "\n",
    "First, we set up the environment as described in our report. This involves initializing the `QandA` object with the `gemma3` model, which our evaluation identified as the best-performing choice for general-purpose extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from qanda import QandA\n",
    "from scores import calculate_bertscore_df\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "FILE_PATH = Path(\"jsondata/Rodier-Finding.jsonl\")\n",
    "GEN_MODEL = \"gemma3\"\n",
    "EMBED_MODEL = \"mxbai-embed-large\"\n",
    "VDB = InMemoryVectorStore\n",
    "TOP_K = 3\n",
    "PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Context information is below.\\n\n",
    "    ---------------------\\n\n",
    "    {context}\\n\n",
    "    ---------------------\\n\n",
    "    Given the context information and not prior knowledge, answer the query.\\n\n",
    "    Query: {input}\\n\n",
    "    Answer:\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing, please wait...\n",
      "Loading jsondata\\Rodier-Finding.jsonl\n",
      "Question Answer chain ready.\n"
     ]
    }
   ],
   "source": [
    "qanda = QandA(gen_model=GEN_MODEL,\n",
    "              embed_model=EMBED_MODEL, \n",
    "              vdb=VDB,\n",
    "              file_path=FILE_PATH,\n",
    "              top_k=TOP_K,\n",
    "              prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantitative Evaluation (Report Section 4)\n",
    "\n",
    "This section reproduces the automated evaluation detailed in our report. We use a predefined set of questions and their corresponding 'ground truth' answers to quantitatively assess the model's performance using BERTScore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation Data\n",
    "\n",
    "Here we define the list of questions and the manually verified correct answers used for scoring. This corresponds to the methodology described in **Section 4.1** of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"Who is the coroner?\", \n",
    "    \"Who is the deceased?\", \n",
    "    \"What was the cause of death?\"\n",
    "]\n",
    "\n",
    "CORRECT_ANSWERS = [\n",
    "    \"Sarah Helen Linton\",\n",
    "    \"Frank Edward Rodier\",\n",
    "    \"unascertained\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating and Scoring Model Answers\n",
    "\n",
    "We now programmatically ask each question, collect the model's answer, and then use the `calculate_bertscore_df` function from our `scores.py` module to generate the final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LLM answers for evaluation...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating LLM answers for evaluation...\")\n",
    "llm_answers = [qanda.ask(q) for q in QUESTIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51c65a8148f41b6ae1615427f17fb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abed9503b87147089fe97ca6f67aa7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.72 seconds, 1.74 sentences/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.45 seconds, 2.07 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'FILENAME': [FILE_PATH.stem] * len(QUESTIONS),\n",
    "    'MODEL': [GEN_MODEL] * len(QUESTIONS),\n",
    "    'QUESTION': QUESTIONS,\n",
    "    'CORRECT_ANSWER': CORRECT_ANSWERS,\n",
    "    'LLM_ANSWER': llm_answers\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scores_df = calculate_bertscore_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation Results\n",
    "\n",
    "The table below presents the final BERT scores for the `gemma3` model on the sample questions. This table provides the empirical evidence for the performance metrics cited in **Table 1** and the analysis in **Section 4.4** of our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>QUESTION</th>\n",
       "      <th>CORRECT_ANSWER</th>\n",
       "      <th>LLM_ANSWER</th>\n",
       "      <th>BERT_PRECISION</th>\n",
       "      <th>BERT_RECALL</th>\n",
       "      <th>BERT_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rodier-Finding</td>\n",
       "      <td>gemma3</td>\n",
       "      <td>Who is the coroner?</td>\n",
       "      <td>Sarah Helen Linton</td>\n",
       "      <td>Sarah Helen Linton, Deputy State Coroner</td>\n",
       "      <td>0.876953</td>\n",
       "      <td>0.963977</td>\n",
       "      <td>0.918408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rodier-Finding</td>\n",
       "      <td>gemma3</td>\n",
       "      <td>Who is the deceased?</td>\n",
       "      <td>Frank Edward Rodier</td>\n",
       "      <td>Frank Edward Rodier is the deceased.</td>\n",
       "      <td>0.913599</td>\n",
       "      <td>0.961740</td>\n",
       "      <td>0.937052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rodier-Finding</td>\n",
       "      <td>gemma3</td>\n",
       "      <td>What was the cause of death?</td>\n",
       "      <td>unascertained</td>\n",
       "      <td>The cause of death remains unascertained. The ...</td>\n",
       "      <td>0.811553</td>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.825647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FILENAME   MODEL                      QUESTION       CORRECT_ANSWER  \\\n",
       "0  Rodier-Finding  gemma3           Who is the coroner?   Sarah Helen Linton   \n",
       "1  Rodier-Finding  gemma3          Who is the deceased?  Frank Edward Rodier   \n",
       "2  Rodier-Finding  gemma3  What was the cause of death?        unascertained   \n",
       "\n",
       "                                          LLM_ANSWER  BERT_PRECISION  \\\n",
       "0           Sarah Helen Linton, Deputy State Coroner        0.876953   \n",
       "1               Frank Edward Rodier is the deceased.        0.913599   \n",
       "2  The cause of death remains unascertained. The ...        0.811553   \n",
       "\n",
       "   BERT_RECALL   BERT_F1  \n",
       "0     0.963977  0.918408  \n",
       "1     0.961740  0.937052  \n",
       "2     0.840240  0.825647  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis of Results\n",
    "\n",
    "As discussed in the report, these results highlight the model's strong performance, particularly its high precision and recall on direct factual questions (e.g., F1-scores > 0.9). This quantitative data supports our conclusion that the Gemma model is the most suitable choice for the WACRSR's goal of building a reliable, fact-based database from coroner reports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
